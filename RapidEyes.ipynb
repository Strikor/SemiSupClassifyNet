{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import hdbscan\n",
    "from collections import defaultdict, Counter       \n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "EPOCHS_SIMCLR = 50     # Warmup 50\n",
    "# Using rounded to balance between hard rounds and random rounds\n",
    "TOTAL_TRIPLET_EPOCHS = 50 # 50\n",
    "MARGIN = 1.0           # Distance between nodes needed to be clustered\n",
    "RANDOM_EPOCHS_PER_CYCLE = 5\n",
    "HARD_EPOCHS_PER_CYCLE = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 2. Data Loading & Transformations (CIFAR-100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# train_dataset_raw = datasets.CIFAR100(\n",
    "#     root=\"./data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=train_transform\n",
    "# )\n",
    "test_dataset_raw = datasets.CIFAR100(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "simclr_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CIFAR 100: print the super classes and fine classes\n",
    " \n",
    "# Load CIFAR-100 metadata\n",
    "def load_cifar100_metadata(meta_path='./data/cifar-100-python/meta'):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "    return data['fine_label_names'], data['coarse_label_names'], data\n",
    "\n",
    "fine_label_names, coarse_label_names, meta_raw = load_cifar100_metadata()\n",
    "\n",
    "# Print each superclass and its fine classes on the same line\n",
    "print(\"CIFAR-100 Superclasses and Their Fine Classes:\\n\")\n",
    "\n",
    "# Each fine label has a fixed superclass index (length 100 list)\n",
    "coarse_label_of_fine = meta_raw['coarse_label_names']\n",
    "superclass_to_fine_names = {name: [] for name in coarse_label_names}\n",
    "\n",
    "# Manually build the map using the known order of classes (fine 0-99)\n",
    "with open('./data/cifar-100-python/train', 'rb') as f:\n",
    "    train_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "for coarse, fine in zip(train_data['coarse_labels'], train_data['fine_labels']):\n",
    "    superclass_name = coarse_label_names[coarse]\n",
    "    subclass_name = fine_label_names[fine]\n",
    "    if subclass_name not in superclass_to_fine_names[superclass_name]:\n",
    "        superclass_to_fine_names[superclass_name].append(subclass_name)\n",
    "\n",
    "# Pretty print\n",
    "for superclass, subclasses in superclass_to_fine_names.items():\n",
    "    print(f\"{superclass}: {', '.join(subclasses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 super classes and one class from each as our training set\n",
    "\n",
    "# Map each coarse label ID to its fine label IDs\n",
    "def build_coarse_to_fine_map():\n",
    "    with open('./data/cifar-100-python/train', 'rb') as f:\n",
    "        train_data = pickle.load(f, encoding='latin1')\n",
    "    coarse_to_fine = {}\n",
    "    for coarse, fine in zip(train_data['coarse_labels'], train_data['fine_labels']):\n",
    "        if coarse not in coarse_to_fine:\n",
    "            coarse_to_fine[coarse] = set()\n",
    "        coarse_to_fine[coarse].add(fine)\n",
    "    return coarse_to_fine\n",
    "\n",
    "coarse_to_fine = build_coarse_to_fine_map()\n",
    "\n",
    "# Step 1: Randomly pick 10 superclasses\n",
    "random.seed(42)\n",
    "selected_coarse_ids = random.sample(list(coarse_to_fine.keys()), 10)\n",
    "print(\"\\nSelected Superclasses:\")\n",
    "for c in selected_coarse_ids:\n",
    "    print(f\"{c}: {coarse_label_names[c]}\")\n",
    "\n",
    "# Step 2: Pick one fine class from each\n",
    "selected_fine_ids = [random.choice(list(coarse_to_fine[c])) for c in selected_coarse_ids]\n",
    "print(\"\\nSelected Fine Classes:\")\n",
    "for f in selected_fine_ids:\n",
    "    print(f\"{f}: {fine_label_names[f]}\")\n",
    "\n",
    "# Step 3: Load and filter CIFAR-100 training set\n",
    "train_dataset_raw = datasets.CIFAR100(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "train_indices = [i for i, label in enumerate(train_dataset_raw.targets) if label in selected_fine_ids]\n",
    "train_dataset_filtered = Subset(train_dataset_raw, train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 3. Supervised Warm-Up (SIMple Contrastive LeaRning SimCLR-like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRDataset(Dataset):\n",
    "    #Takes the same image and mutates it in some way\n",
    "    #While telling the network they're the same\n",
    "    #Pulls out general features\n",
    "    def __init__(self, base_dataset, transform=None, second_transform=None):\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "        self.second_transform = second_transform if second_transform else transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_img = self.base_dataset.data[idx]\n",
    "        pil_img = Image.fromarray(raw_img)\n",
    "        img1 = self.transform(pil_img)\n",
    "        img2 = self.second_transform(pil_img)\n",
    "        return img1, img2\n",
    "\n",
    "\n",
    "class SimpleConvBase(nn.Module):\n",
    "    #Extreamly basic CNN, probably need to make it more complex for better results\n",
    "    def __init__(self, out_dim=128):\n",
    "        super(SimpleConvBase, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  #16x16\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  #8x8\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(128, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimCLRModel(nn.Module):\n",
    "    #Adds a projection head to the existing CNN to allow contrastive loss to function better\n",
    "    def __init__(self, base_out_dim=128, projection_dim=64):\n",
    "        super(SimCLRModel, self).__init__()\n",
    "        self.encoder = SimpleConvBase(out_dim=base_out_dim)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(base_out_dim, base_out_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(base_out_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        proj = self.projection(feat)\n",
    "        return feat, proj\n",
    "\n",
    "\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "    #NT-Xent is a special loss function specifally to facilitate\n",
    "    #Moving samples toward or away from one another\n",
    "    batch_size = z_i.size(0)\n",
    "    z_i = F.normalize(z_i, dim=1)\n",
    "    z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "    logits = torch.matmul(z_i, z_j.t()) / temperature\n",
    "    labels = torch.arange(batch_size).long().to(z_i.device)\n",
    "\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_j = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i + loss_j) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simclr_epoch(model, loader, optimizer, epoch, temperature=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img1, img2 in loader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, z1 = model(img1)\n",
    "        _, z2 = model(img2)\n",
    "\n",
    "        loss = nt_xent_loss(z1, z2, temperature=temperature)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"[SimCLR E{epoch}] Loss={avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "simclr_dataset = SimCLRDataset(\n",
    "    base_dataset=train_dataset_raw,\n",
    "    transform=simclr_transform,\n",
    "    second_transform=simclr_transform\n",
    ")\n",
    "\n",
    "simclr_loader = DataLoader(\n",
    "    simclr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "simclr_model = SimCLRModel(base_out_dim=128, projection_dim=64).to(device)\n",
    "optimizer_simclr = optim.Adam(simclr_model.parameters(), lr=LR)\n",
    "\n",
    "print(f\"SimCLR Warm-Up Training with {EPOCHS_SIMCLR} Epochs\")\n",
    "for e in range(1, EPOCHS_SIMCLR + 1):\n",
    "    train_simclr_epoch(simclr_model, simclr_loader, optimizer_simclr, epoch=e, temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 4. Triplet Network Training & Distance Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    #Augmenting the existing CNN to use the triplet system\n",
    "    def __init__(self, encoder):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        a = self.get_embedding(anchor)\n",
    "        p = self.get_embedding(positive)\n",
    "        n = self.get_embedding(negative)\n",
    "        return a, p, n\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        #Get raw CNN output\n",
    "        raw_emb = self.encoder(x)\n",
    "        #Normalize to unit sphere\n",
    "        emb = F.normalize(raw_emb, p=2, dim=1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "def build_embedding_cache(model, dataset):\n",
    "    #Store image embeddings rather than constantly recalculating\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "    cache_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (imgs, _) in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            emb_batch = model.get_embedding(imgs).cpu().numpy()\n",
    "            cache_list.append(emb_batch)\n",
    "    return np.concatenate(cache_list, axis=0)\n",
    "\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    #If hard_negative is set the system will select a negative which is located nearby\n",
    "    #in order to challenge the network to find more underlying features\n",
    "    def __init__(self, base_dataset, embedding_cache=None, hard_negative=False, K=20):\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        self.labels = np.array(base_dataset.targets)\n",
    "        self.class_to_indices = {}\n",
    "        for idx, lab in enumerate(self.labels):\n",
    "            if lab not in self.class_to_indices:\n",
    "                self.class_to_indices[lab] = []\n",
    "            self.class_to_indices[lab].append(idx)\n",
    "\n",
    "        self.embedding_cache = embedding_cache\n",
    "        self.hard_negative = hard_negative\n",
    "        self.K = K  # top-K nearest for negatives\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_img, anchor_label = self.base_dataset[index]\n",
    "\n",
    "        #Positive\n",
    "        pos_index = index\n",
    "        while pos_index == index:\n",
    "            pos_index = random.choice(self.class_to_indices[anchor_label])\n",
    "        pos_img, _ = self.base_dataset[pos_index]\n",
    "\n",
    "        #Negative\n",
    "        if not self.hard_negative:\n",
    "            #Random negative from another class\n",
    "            neg_label = anchor_label\n",
    "            while neg_label == anchor_label:\n",
    "                neg_label = random.choice(list(self.class_to_indices.keys()))\n",
    "            neg_index = random.choice(self.class_to_indices[neg_label])\n",
    "        else:\n",
    "            #Semi-hard negative from top-K nearest\n",
    "            anchor_emb = self.embedding_cache[index]\n",
    "            dists = np.sum((self.embedding_cache - anchor_emb)**2, axis=1)\n",
    "            nearest_indices = np.argsort(dists)[1:self.K+1]\n",
    "            candidate_neg = [i_n for i_n in nearest_indices if self.labels[i_n] != anchor_label]\n",
    "            if len(candidate_neg) == 0:\n",
    "                #Ifhard negative can't be found resort to random\n",
    "                neg_label = anchor_label\n",
    "                while neg_label == anchor_label:\n",
    "                    neg_label = random.choice(list(self.class_to_indices.keys()))\n",
    "                neg_index = random.choice(self.class_to_indices[neg_label])\n",
    "            else:\n",
    "                neg_index = random.choice(candidate_neg)\n",
    "\n",
    "        neg_img, _ = self.base_dataset[neg_index]\n",
    "        return anchor_img, pos_img, neg_img, anchor_label, anchor_label, self.labels[neg_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_triplet(model, loader, optimizer, margin=MARGIN):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (anchor_img, pos_img, neg_img, _, _, _) in loader:\n",
    "        anchor_img = anchor_img.to(device)\n",
    "        pos_img = pos_img.to(device)\n",
    "        neg_img = neg_img.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        a, p, n = model(anchor_img, pos_img, neg_img)\n",
    "\n",
    "        dist_pos = F.pairwise_distance(a, p, p=2)\n",
    "        dist_neg = F.pairwise_distance(a, n, p=2)\n",
    "        loss = torch.clamp(dist_pos - dist_neg + margin, min=0).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def quick_distance_check(model, dataset, n_pairs=1000):\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:2000]\n",
    "    loader = DataLoader(Subset(dataset, indices), batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "    emb_list = []\n",
    "    lbl_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            emb = model.get_embedding(imgs).cpu().numpy()\n",
    "            emb_list.append(emb)\n",
    "            lbl_list.append(labels.numpy())\n",
    "\n",
    "    embeddings = np.concatenate(emb_list, axis=0)\n",
    "    all_labels = np.concatenate(lbl_list, axis=0)\n",
    "    n_data = embeddings.shape[0]\n",
    "\n",
    "    same_dists = []\n",
    "    diff_dists = []\n",
    "    for _ in range(n_pairs):\n",
    "        i1, i2 = np.random.randint(0, n_data, size=2)\n",
    "        dist = np.linalg.norm(embeddings[i1] - embeddings[i2])\n",
    "        if all_labels[i1] == all_labels[i2]:\n",
    "            same_dists.append(dist)\n",
    "        else:\n",
    "            diff_dists.append(dist)\n",
    "\n",
    "    if len(same_dists) > 0 and len(diff_dists) > 0:\n",
    "        print(f\"[DistCheck] same={np.mean(same_dists):.4f}, diff={np.mean(diff_dists):.4f}\")\n",
    "    else:\n",
    "        print(\"Not enough pairs to measure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model = TripletNet(simclr_model.encoder).to(device)\n",
    "optimizer_triplet = optim.Adam(triplet_model.parameters(), lr=LR)\n",
    "\n",
    "print(f\"Spliced Triplet Training with {TOTAL_TRIPLET_EPOCHS} Epochs\")\n",
    "current_epoch = 1\n",
    "epochs_left = TOTAL_TRIPLET_EPOCHS\n",
    "\n",
    "while epochs_left > 0:\n",
    "    #Random-negative epochs\n",
    "    for i in range(min(epochs_left, RANDOM_EPOCHS_PER_CYCLE)):\n",
    "        random_dataset = TripletDataset(\n",
    "            base_dataset=train_dataset_raw,\n",
    "            embedding_cache=None,\n",
    "            hard_negative=False\n",
    "        )\n",
    "        random_loader = DataLoader(\n",
    "            random_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        loss_r = train_one_epoch_triplet(triplet_model, random_loader, optimizer_triplet, margin=MARGIN)\n",
    "        print(f\"[Epoch {current_epoch}] RandomNeg Loss={loss_r:.4f}\")\n",
    "        quick_distance_check(triplet_model, train_dataset_raw)\n",
    "\n",
    "        current_epoch += 1\n",
    "        epochs_left -= 1\n",
    "        if epochs_left <= 0:\n",
    "            break\n",
    "\n",
    "    if epochs_left <= 0:\n",
    "        break\n",
    "\n",
    "    #Hard-negative (semi-hard) epoch\n",
    "    embed_cache = build_embedding_cache(triplet_model, train_dataset_raw)\n",
    "    hard_dataset = TripletDataset(\n",
    "        base_dataset=train_dataset_raw,\n",
    "        embedding_cache=embed_cache,\n",
    "        hard_negative=True,\n",
    "        K=20  #pick from top-20 nearest\n",
    "    )\n",
    "    hard_loader = DataLoader(\n",
    "        hard_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    loss_h = train_one_epoch_triplet(triplet_model, hard_loader, optimizer_triplet, margin=MARGIN)\n",
    "    print(f\"[Epoch {current_epoch}] HardNeg Loss={loss_h:.4f}\")\n",
    "    quick_distance_check(triplet_model, train_dataset_raw)\n",
    "\n",
    "    current_epoch += 1\n",
    "    epochs_left -= 1\n",
    "\n",
    "print(\"Finished Spliced Triplet Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# **Save & Load Model Code Block**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_triplet_model(model, optimizer, epoch, filepath=\"triplet_model_checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    model: TripletNet instance\n",
    "    optimizer: optimizer_triplet\n",
    "    epoch: current epoch or training step\n",
    "    filepath: where to save\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict()\n",
    "    }, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_triplet_model(model, optimizer, filepath=\"triplet_model_checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Load states into 'model' and 'optimizer'.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Model loaded from {filepath}, resume from epoch {start_epoch}\")\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save the model ===\n",
    "save_triplet_model(triplet_model, optimizer_triplet, current_epoch, \"triplet_model_checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load the model ===\n",
    "resume_epoch = load_triplet_model(triplet_model, optimizer_triplet, \"triplet_model_checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# 5. Data Visualization & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_visuals(model, dataset, max_images=2000):\n",
    "    \"\"\"\n",
    "    Generates PCA, UMAP, and t-SNE visualizations on a subset of 'dataset'.\n",
    "    Colored according to ground-truth labels for reference.\n",
    "    \"\"\"\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:max_images]\n",
    "\n",
    "    sub_loader = DataLoader(\n",
    "        Subset(dataset, indices),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    emb_list = []\n",
    "    lbl_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in sub_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            emb_batch = model.get_embedding(imgs).cpu().numpy()\n",
    "            emb_list.append(emb_batch)\n",
    "            lbl_list.append(labels.numpy())\n",
    "\n",
    "    embeddings = np.concatenate(emb_list, axis=0)\n",
    "    labels_np = np.concatenate(lbl_list, axis=0)\n",
    "\n",
    "    #Principal Componenet Analysis\n",
    "    #Finds directionality in data with the greatest variance\n",
    "    #Simplified it \"kinda\" maps features to a directional vector and each feature pushes\n",
    "    #a data point in a direction\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    emb_pca = pca.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(emb_pca[:,0], emb_pca[:,1], c=labels_np, cmap='tab20', s=5)\n",
    "    plt.title(\"PCA (final embeddings, subset)\")\n",
    "    plt.show()\n",
    "\n",
    "    #Uniform Manifold Approximation and Projection\n",
    "    #Maps data based on how likely or unlikely each point is to be close to\n",
    "    #each other then maps this fuzzy distance understanding to 2d space\n",
    "    reducer = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.1, random_state=42)\n",
    "    emb_umap = reducer.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(emb_umap[:,0], emb_umap[:,1], c=labels_np, cmap='tab20', s=5)\n",
    "    plt.title(\"UMAP (final embeddings, subset)\")\n",
    "    plt.show()\n",
    "\n",
    "    #t-Distributed Stochastic Neighbor Embeddings\n",
    "    #Creates \"neighborhoods\" to find groups of data that would be close to one another\n",
    "    #Then uses this fuzzy understanding to group data together\n",
    "    tsne_model = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    emb_tsne = tsne_model.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(emb_tsne[:,0], emb_tsne[:,1], c=labels_np, cmap='tab20', s=5)\n",
    "    plt.title(\"t-SNE (final embeddings, subset)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cluster_test_set_hdbscan(model, dataset, max_images=2000):\n",
    "    #Clusters dataset with HBDSCAN\n",
    "    #Prints ARI and NMI for debuging purposes\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:max_images]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        Subset(dataset, indices),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    emb_list = []\n",
    "    lbl_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            emb = model.get_embedding(imgs).cpu().numpy()\n",
    "            emb_list.append(emb)\n",
    "            lbl_list.append(lbls.numpy())\n",
    "\n",
    "    embeddings = np.concatenate(emb_list, axis=0)\n",
    "    labels_np = np.concatenate(lbl_list, axis=0)\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=30)\n",
    "    preds = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    n_clusters = len(set(preds) - {-1})\n",
    "    print(f\"[HDBSCAN] Found {n_clusters} clusters (excluding noise label=-1).\")\n",
    "\n",
    "    valid_idx = preds != -1\n",
    "    if np.sum(valid_idx) > 0:\n",
    "        ari = adjusted_rand_score(labels_np[valid_idx], preds[valid_idx])\n",
    "        nmi = normalized_mutual_info_score(labels_np[valid_idx], preds[valid_idx])\n",
    "        print(f\" ARI={ari:.4f}, NMI={nmi:.4f}\")\n",
    "    else:\n",
    "        print(\" No valid (non-noise) clusters for ARI/NMI.\")\n",
    "\n",
    "    # UMAP scatter for cluster visualization\n",
    "    reducer = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.5, random_state=42)\n",
    "    emb_umap = reducer.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(emb_umap[:,0], emb_umap[:,1], c=preds, cmap='Spectral', s=5)\n",
    "    plt.title(\"UMAP of Test Embeddings colored by HDBSCAN clusters\")\n",
    "    plt.show()\n",
    "\n",
    "embedding_visuals(triplet_model, test_dataset_raw, max_images=2000)\n",
    "cluster_test_set_hdbscan(triplet_model, test_dataset_raw, max_images=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# 6. Select & Cluster Initial Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledSubset(Dataset):\n",
    "    #Limits the amount of classes that can be used in the dataset\n",
    "    def __init__(self, base_dataset, max_images=5000, max_classes=10):\n",
    "        self.base_dataset = base_dataset\n",
    "        labels_array = np.array(base_dataset.targets)\n",
    "        unique_labels = np.unique(labels_array)\n",
    "        random.shuffle(unique_labels)\n",
    "        chosen_labels = unique_labels[:max_classes]\n",
    "\n",
    "        indices = []\n",
    "        for cl in chosen_labels:\n",
    "            these_idx = np.where(labels_array == cl)[0]\n",
    "            indices.extend(these_idx.tolist())\n",
    "        random.shuffle(indices)\n",
    "        indices = indices[:max_images]\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        img, lbl = self.base_dataset[real_idx]\n",
    "        return img, lbl\n",
    "\n",
    "print(\"Selecting subset images from limited classes\")\n",
    "\n",
    "# Replace 'train_dataset_raw' with base dataset\n",
    "unlabeled_init_dataset = UnlabeledSubset(train_dataset_raw, max_images=5000, max_classes=10)\n",
    "print(f\"  => Subset size: {len(unlabeled_init_dataset)} images\")\n",
    "\n",
    "# Build embeddings for subset\n",
    "loader_init = DataLoader(unlabeled_init_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "init_emb_list = []\n",
    "init_labels_list = []\n",
    "triplet_model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in loader_init:\n",
    "        imgs = imgs.to(device)\n",
    "        emb_batch = triplet_model.get_embedding(imgs).cpu().numpy()\n",
    "        init_emb_list.append(emb_batch)\n",
    "        init_labels_list.extend(lbls.numpy())\n",
    "\n",
    "init_embeddings = np.concatenate(init_emb_list, axis=0)\n",
    "init_labels = np.array(init_labels_list)\n",
    "\n",
    "clusterer_init = hdbscan.HDBSCAN(min_cluster_size=50)\n",
    "init_preds = clusterer_init.fit_predict(init_embeddings)\n",
    "unique_clusters_init = set(init_preds) - {-1}\n",
    "\n",
    "cluster_buckets = {}\n",
    "for cid in unique_clusters_init:\n",
    "    cluster_buckets[cid] = []\n",
    "cluster_buckets['noise'] = []\n",
    "\n",
    "for i, c in enumerate(init_preds):\n",
    "    if c == -1:\n",
    "        cluster_buckets['noise'].append(i)\n",
    "    else:\n",
    "        cluster_buckets[c].append(i)\n",
    "\n",
    "print(\"Initial Clusters\")\n",
    "for k in cluster_buckets:\n",
    "    print(f\"  Cluster {k}: size={len(cluster_buckets[k])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# 7. StreamingClusterManager + GravityTripletDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GravityTripletDataset(Dataset):\n",
    "    def __init__(self, model, device, all_embeddings, base_dataset, triplets_info):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.all_embeddings = all_embeddings\n",
    "        self.base_dataset = base_dataset\n",
    "        self.triplets_info = triplets_info\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_img, pos_centroid, neg_centroid = self.triplets_info[idx]\n",
    "        img, _ = self.base_dataset[idx_img]\n",
    "        return (\n",
    "            img,\n",
    "            torch.tensor(pos_centroid, dtype=torch.float32),\n",
    "            torch.tensor(neg_centroid, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "class StreamingClusterManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        device,\n",
    "        init_embeddings,\n",
    "        init_preds,\n",
    "        init_labels,\n",
    "        cluster_buckets,\n",
    "        distance_threshold=1.5\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.distance_threshold = distance_threshold\n",
    "        self.all_embeddings = init_embeddings.copy()\n",
    "        self.all_cluster_ids = init_preds.copy()\n",
    "        self.all_labels = init_labels.copy()\n",
    "        self.num_initial = self.all_embeddings.shape[0]\n",
    "        \n",
    "        self.cluster_buckets = {}\n",
    "        for cid in cluster_buckets:\n",
    "            self.cluster_buckets[cid] = []\n",
    "        for cid in cluster_buckets:\n",
    "            for idx_val in cluster_buckets[cid]:\n",
    "                if idx_val < self.num_initial:\n",
    "                    self.cluster_buckets[cid].append(idx_val)\n",
    "\n",
    "        self.centroids = {}\n",
    "        for cid in self.cluster_buckets:\n",
    "            if len(self.cluster_buckets[cid]) > 0:\n",
    "                emb_sub = self.all_embeddings[self.cluster_buckets[cid]]\n",
    "                centroid = emb_sub.mean(axis=0)\n",
    "                self.centroids[cid] = centroid\n",
    "\n",
    "    def assign_new_image(self, img_tensor, true_label):\n",
    "        if img_tensor.ndim == 3:\n",
    "            img_tensor = img_tensor.unsqueeze(0)\n",
    "        img_tensor = img_tensor.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            emb = self.model.get_embedding(img_tensor).cpu().numpy()[0]\n",
    "\n",
    "        if len(self.centroids) == 0:\n",
    "            new_cid = 0\n",
    "            self.cluster_buckets[new_cid] = [self.num_initial]\n",
    "            self.all_cluster_ids = np.append(self.all_cluster_ids, new_cid)\n",
    "            self.all_embeddings = np.vstack([self.all_embeddings, emb])\n",
    "            self.all_labels = np.append(self.all_labels, true_label)\n",
    "            self.centroids[new_cid] = emb\n",
    "            self.num_initial += 1\n",
    "            return (new_cid, 0.0)\n",
    "\n",
    "        dists = {}\n",
    "        for cid, cvec in self.centroids.items():\n",
    "            dists[cid] = np.linalg.norm(emb - cvec)\n",
    "        nearest_cid = min(dists, key=dists.get)\n",
    "        nearest_dist = dists[nearest_cid]\n",
    "\n",
    "        if nearest_dist > self.distance_threshold:\n",
    "            new_cid = max([x for x in self.centroids.keys() if isinstance(x, int)], default=-1) + 1\n",
    "            self.cluster_buckets[new_cid] = [self.num_initial]\n",
    "            self.all_cluster_ids = np.append(self.all_cluster_ids, new_cid)\n",
    "            self.all_embeddings = np.vstack([self.all_embeddings, emb])\n",
    "            self.all_labels = np.append(self.all_labels, true_label)\n",
    "            self.centroids[new_cid] = emb\n",
    "            self.num_initial += 1\n",
    "            return (new_cid, 0.0)\n",
    "        else:\n",
    "            idx_new = self.num_initial\n",
    "            self.num_initial += 1\n",
    "            self.all_cluster_ids = np.append(self.all_cluster_ids, nearest_cid)\n",
    "            self.all_embeddings = np.vstack([self.all_embeddings, emb])\n",
    "            self.all_labels = np.append(self.all_labels, true_label)\n",
    "            self.cluster_buckets[nearest_cid].append(idx_new)\n",
    "\n",
    "            old_count = len(self.cluster_buckets[nearest_cid]) - 1\n",
    "            old_centroid = self.centroids[nearest_cid]\n",
    "            new_count = old_count + 1\n",
    "            new_centroid = (old_centroid * old_count + emb) / new_count\n",
    "            self.centroids[nearest_cid] = new_centroid\n",
    "            return (nearest_cid, nearest_dist)\n",
    "\n",
    "    def periodic_recluster(self):\n",
    "        if self.all_embeddings.shape[0] == 0:\n",
    "            return\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=50)\n",
    "        new_preds = clusterer.fit_predict(self.all_embeddings)\n",
    "        # Possibly reassign or merge clusters here if desired.\n",
    "\n",
    "    def gravity_retrain(\n",
    "        self,\n",
    "        optimizer,\n",
    "        unlabeled_dataset,\n",
    "        margin=1.0,\n",
    "        epochs=3,\n",
    "        outlier_cid=\"noise\"\n",
    "    ):\n",
    "        all_triplets = []\n",
    "        cluster_ids_list = sorted([c for c in self.cluster_buckets.keys() if isinstance(c, int)])\n",
    "        if outlier_cid in self.cluster_buckets:\n",
    "            cluster_ids_list.append(outlier_cid)\n",
    "\n",
    "        for cid in cluster_ids_list:\n",
    "            if cid not in self.cluster_buckets:\n",
    "                continue\n",
    "            indices_c = self.cluster_buckets[cid]\n",
    "            if len(indices_c) < 10:\n",
    "                continue\n",
    "            random.shuffle(indices_c)\n",
    "            sample_indices = indices_c[:50]\n",
    "            other_cids = [x for x in cluster_ids_list if x != cid and x in self.cluster_buckets]\n",
    "            if len(other_cids) == 0:\n",
    "                continue\n",
    "            neg_cid = random.choice(other_cids)\n",
    "            if neg_cid not in self.centroids:\n",
    "                continue\n",
    "            if cid not in self.centroids:\n",
    "                continue\n",
    "            neg_centroid = self.centroids[neg_cid]\n",
    "            pos_centroid = self.centroids[cid]\n",
    "            for idx_img in sample_indices:\n",
    "                all_triplets.append((idx_img, pos_centroid, neg_centroid))\n",
    "\n",
    "        ds = GravityTripletDataset(\n",
    "            self.model,\n",
    "            self.device,\n",
    "            self.all_embeddings,\n",
    "            unlabeled_dataset,\n",
    "            all_triplets\n",
    "        )\n",
    "        if len(ds) == 0:\n",
    "            return\n",
    "        loader = DataLoader(ds, batch_size=32, shuffle=True, pin_memory=True)\n",
    "        for _ in range(epochs):\n",
    "            self.model.train()\n",
    "            for anchor_img, pos_vec, neg_vec in loader:\n",
    "                anchor_img = anchor_img.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                a_emb = self.model.get_embedding(anchor_img)\n",
    "                pos_vec = pos_vec.to(self.device)\n",
    "                neg_vec = neg_vec.to(self.device)\n",
    "                dist_pos = F.pairwise_distance(a_emb, pos_vec, p=2)\n",
    "                dist_neg = F.pairwise_distance(a_emb, neg_vec, p=2)\n",
    "                loss = torch.clamp(dist_pos - dist_neg + margin, min=0).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def iterative_streaming_cycle(\n",
    "        self,\n",
    "        new_dataset,\n",
    "        optimizer,\n",
    "        unlabeled_dataset,\n",
    "        cycle_size=1000,\n",
    "        total_images=5000,\n",
    "        margin=1.0,\n",
    "        retrain_epochs=3,\n",
    "        outlier_cid=\"noise\"\n",
    "    ):\n",
    "        images_streamed = 0\n",
    "        while images_streamed < total_images:\n",
    "            batch_end = min(images_streamed + cycle_size, total_images)\n",
    "            for i in range(images_streamed, batch_end):\n",
    "                if i >= len(new_dataset):\n",
    "                    break\n",
    "                img_tensor, lbl = new_dataset[i]\n",
    "                self.assign_new_image(img_tensor, lbl)\n",
    "            images_streamed = batch_end\n",
    "            self.gravity_retrain(\n",
    "                optimizer=optimizer,\n",
    "                unlabeled_dataset=unlabeled_dataset,\n",
    "                margin=margin,\n",
    "                epochs=retrain_epochs,\n",
    "                outlier_cid=outlier_cid\n",
    "            )\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        valid_mask = [i for i in range(len(self.all_cluster_ids)) if self.all_cluster_ids[i] != -1]\n",
    "        assigned_clusters = self.all_cluster_ids[valid_mask]\n",
    "        assigned_labels = self.all_labels[valid_mask]\n",
    "        if len(set(assigned_clusters)) <= 1:\n",
    "            return\n",
    "        ari_val = adjusted_rand_score(assigned_labels, assigned_clusters)\n",
    "        nmi_val = normalized_mutual_info_score(assigned_labels, assigned_clusters)\n",
    "        print(f\"ARI={ari_val:.4f}, NMI={nmi_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# 8. Visualization, Loop & Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(manager, max_points=2000):\n",
    "    if manager.all_embeddings.shape[0] == 0:\n",
    "        return\n",
    "    if manager.all_embeddings.shape[0] > max_points:\n",
    "        idx_subset = np.random.choice(manager.all_embeddings.shape[0], max_points, replace=False)\n",
    "        emb_subset = manager.all_embeddings[idx_subset]\n",
    "        clust_subset = manager.all_cluster_ids[idx_subset]\n",
    "    else:\n",
    "        emb_subset = manager.all_embeddings\n",
    "        clust_subset = manager.all_cluster_ids\n",
    "\n",
    "    # Convert outliers to -1 cluster to standardize to int\n",
    "    clust_subset_str = clust_subset.astype(str)\n",
    "    clust_subset_str[clust_subset_str == 'noise'] = '-1'\n",
    "    clust_subset_str[clust_subset_str == '-1'] = '-1'\n",
    "    clust_subset_int = clust_subset_str.astype(int)\n",
    "\n",
    "    tsne_model = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    emb_tsne = tsne_model.fit_transform(emb_subset)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(emb_tsne[:, 0], emb_tsne[:, 1], c=clust_subset_int, cmap='tab20', s=5)\n",
    "    plt.title(\"t-SNE Clusters\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cluster_counts(manager):\n",
    "    # Include np.integer to account for NumPy int64 keys\n",
    "    cluster_ids_sorted = sorted(\n",
    "        c for c in manager.cluster_buckets.keys()\n",
    "        if isinstance(c, (int, np.integer))\n",
    "    )\n",
    "    cluster_sizes = [len(manager.cluster_buckets[c]) for c in cluster_ids_sorted]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar([str(k) for k in cluster_ids_sorted], cluster_sizes, color='skyblue')\n",
    "    plt.xlabel(\"Cluster ID\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Cluster Counts\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_ari_nmi(manager):\n",
    "    valid_mask = [i for i in range(len(manager.all_cluster_ids)) if manager.all_cluster_ids[i] != -1]\n",
    "    assigned_clusters = manager.all_cluster_ids[valid_mask]\n",
    "    assigned_labels = manager.all_labels[valid_mask]\n",
    "\n",
    "    if len(set(assigned_clusters)) <= 1:\n",
    "        return\n",
    "\n",
    "    ari_val = adjusted_rand_score(assigned_labels, assigned_clusters)\n",
    "    nmi_val = normalized_mutual_info_score(assigned_labels, assigned_clusters)\n",
    "    print(f\"ARI={ari_val:.4f}, NMI={nmi_val:.4f}\")\n",
    "\n",
    "\n",
    "def final_summary(manager):\n",
    "    cluster_ids_sorted = sorted(\n",
    "        c for c in manager.cluster_buckets.keys()\n",
    "        if isinstance(c, (int, np.integer))\n",
    "    )\n",
    "\n",
    "    print(\"\\nFinal Cluster Sizes:\")\n",
    "    for cid in cluster_ids_sorted:\n",
    "        print(f\"  Cluster {cid} -> {len(manager.cluster_buckets[cid])} items\")\n",
    "\n",
    "    print(f\"\\nNumber of clusters (excluding 'noise'): {len(cluster_ids_sorted)}\")\n",
    "\n",
    "    #Simple \"dominant label\" approach for each cluster\n",
    "    #Limits definition of accuracy in whats supposed to be \n",
    "    #a general clustering round\n",
    "    print(\"\\nDominant Label (old single-label purity measure):\")\n",
    "    for cid in cluster_ids_sorted:\n",
    "        indices_c = manager.cluster_buckets[cid]\n",
    "        labels_c = manager.all_labels[indices_c]\n",
    "        label_counts = Counter(labels_c)\n",
    "        if len(indices_c) == 0:\n",
    "            print(f\"  Cluster {cid}: 0 items.\")\n",
    "            continue\n",
    "        domin_label, dom_count = label_counts.most_common(1)[0]\n",
    "        purity = dom_count / len(indices_c)\n",
    "        print(f\"  Cluster {cid}: domin_label={domin_label}, purity={purity:.4f}\")\n",
    "\n",
    "    # ARI / NMI on all assigned data\n",
    "    valid_mask = [i for i in range(len(manager.all_cluster_ids)) if manager.all_cluster_ids[i] != -1]\n",
    "    assigned_clusters = manager.all_cluster_ids[valid_mask]\n",
    "    assigned_labels = manager.all_labels[valid_mask]\n",
    "    if len(set(assigned_clusters)) <= 1:\n",
    "        print(\"\\nFinal ARI/NMI not computed (only one cluster).\")\n",
    "    else:\n",
    "        ari_val = adjusted_rand_score(assigned_labels, assigned_clusters)\n",
    "        nmi_val = normalized_mutual_info_score(assigned_labels, assigned_clusters)\n",
    "        print(f\"\\nFinal ARI={ari_val:.4f}, NMI={nmi_val:.4f}\")\n",
    "\n",
    "    #Multi-label Purity for Each Cluster\n",
    "    #For each cluster, list each label contained and the fraction\n",
    "    #of the cluster that label occupies. Summarize them as well.\n",
    "    print(\"\\nMulti-label Breakdown for Each Cluster:\")\n",
    "    for cid in cluster_ids_sorted:\n",
    "        indices_c = manager.cluster_buckets[cid]\n",
    "        if not indices_c:\n",
    "            print(f\"  Cluster {cid}: 0 items.\")\n",
    "            continue\n",
    "        labels_c = manager.all_labels[indices_c]\n",
    "        label_counts = Counter(labels_c)\n",
    "        cluster_size = len(indices_c)\n",
    "\n",
    "        print(f\"  Cluster {cid} (size={cluster_size}):\")\n",
    "        #Sort labels in descending order of count\n",
    "        for lbl_val, lbl_count in label_counts.most_common():\n",
    "            frac = lbl_count / cluster_size\n",
    "            print(f\"    label={lbl_val} => count={lbl_count}, fraction={frac:.4f}\")\n",
    "\n",
    "\n",
    "def long_loop_streaming(manager,\n",
    "                        new_dataset,\n",
    "                        optimizer,\n",
    "                        unlabeled_dataset,\n",
    "                        total_cycles=50,\n",
    "                        images_per_cycle=200,\n",
    "                        margin=1.0,\n",
    "                        retrain_epochs=3,\n",
    "                        outlier_cid=\"noise\"):\n",
    "    images_streamed = 0\n",
    "    for cycle in range(1, total_cycles + 1):\n",
    "        start_idx = images_streamed\n",
    "        end_idx = images_streamed + images_per_cycle\n",
    "        for i in range(start_idx, end_idx):\n",
    "            if i >= len(new_dataset):\n",
    "                break\n",
    "            img_tensor, lbl = new_dataset[i]\n",
    "            manager.assign_new_image(img_tensor, lbl)\n",
    "\n",
    "        images_streamed = end_idx\n",
    "        manager.gravity_retrain(\n",
    "            optimizer=optimizer,\n",
    "            unlabeled_dataset=unlabeled_dataset,\n",
    "            margin=margin,\n",
    "            epochs=retrain_epochs,\n",
    "            outlier_cid=outlier_cid\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCycle {cycle}/{total_cycles} complete. Images streamed so far: {images_streamed}.\")\n",
    "        manager.compute_metrics()\n",
    "        plot_clusters(manager)\n",
    "        plot_cluster_counts(manager)\n",
    "        print_ari_nmi(manager)\n",
    "\n",
    "        if images_streamed >= len(new_dataset):\n",
    "            break\n",
    "\n",
    "    print(\"\\nAll cycles complete.\")\n",
    "    print(\"DEBUG: manager.cluster_buckets keys & sizes:\")\n",
    "    for k in manager.cluster_buckets:\n",
    "        print(f\"  {k} -> {len(manager.cluster_buckets[k])} items\")\n",
    "\n",
    "    for k in manager.cluster_buckets:\n",
    "        print(f\"  {k} -> {len(manager.cluster_buckets[k])} items  (type={type(k)})\")\n",
    "\n",
    "    final_summary(manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the results of Block 9\n",
    "# init_embeddings, init_preds, init_labels, cluster_buckets\n",
    "\n",
    "manager = StreamingClusterManager(\n",
    "    model=triplet_model,\n",
    "    device=device,\n",
    "    init_embeddings=init_embeddings,\n",
    "    init_preds=init_preds,\n",
    "    init_labels=init_labels,\n",
    "    cluster_buckets=cluster_buckets,\n",
    "    distance_threshold=0.5\n",
    ")\n",
    "\n",
    "new_dataset = test_dataset_raw\n",
    "\n",
    "long_loop_streaming(\n",
    "    manager=manager,\n",
    "    new_dataset=new_dataset,\n",
    "    optimizer=optimizer_triplet,\n",
    "    unlabeled_dataset=unlabeled_init_dataset,\n",
    "    total_cycles=50,\n",
    "    images_per_cycle=200,\n",
    "    margin=1.0,\n",
    "    retrain_epochs=3,\n",
    "    outlier_cid=\"noise\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# 9. Final accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Code Block: Evaluate Class-Cluster Assignments\n",
    "#\n",
    "# This function:\n",
    "# 1) Gathers how many images of each label (class) appear in each cluster.\n",
    "# 2) For each label L, identifies the cluster that contains the largest number of L.\n",
    "#    We treat that cluster as the \"correct\" cluster for L.\n",
    "# 3) Counts how many images of L fall outside that \"correct\" cluster (those are errors).\n",
    "# 4) Computes an accuracy measure per label L = (# of L in correct cluster) / (total # of L).\n",
    "# 5) Also computes a per-cluster \"error\" count (how many images inside that cluster actually\n",
    "#    belong to a different label's correct cluster).\n",
    "#\n",
    "# Usage:\n",
    "#   - Paste this at the end of your existing notebook.\n",
    "#   - Call evaluate_class_accuracy(manager) to print results or store them in dicts.\n",
    "################################################################################\n",
    "\n",
    "def evaluate_class_accuracy(manager):\n",
    "\n",
    "    # 'manager.all_cluster_ids' has a cluster ID (or -1 for noise) for each image index.\n",
    "    # 'manager.all_labels' has the corresponding true label for each image index.\n",
    "    cluster_ids = manager.all_cluster_ids\n",
    "    labels = manager.all_labels\n",
    "\n",
    "    # Filter out noise indices if you want to skip them in the analysis\n",
    "    valid_indices = np.where(cluster_ids != -1)[0]\n",
    "    valid_clusters = cluster_ids[valid_indices]\n",
    "    valid_labels   = labels[valid_indices]\n",
    "\n",
    "    # Build a dictionary: (cluster -> Counter of labels)\n",
    "    cluster_label_counts = defaultdict(Counter)\n",
    "    for i, c in zip(valid_indices, valid_clusters):\n",
    "        lbl = labels[i]\n",
    "        cluster_label_counts[c][lbl] += 1\n",
    "\n",
    "    # Build a total label count\n",
    "    total_label_count = Counter(valid_labels)\n",
    "\n",
    "    # 1) For each label L, find the cluster that has the maximum count of L.\n",
    "    best_cluster_for_label = {}\n",
    "    unique_labels = sorted(set(valid_labels))\n",
    "    cluster_ids_sorted = sorted(\n",
    "        c for c in set(valid_clusters) if c != -1\n",
    "    )\n",
    "\n",
    "    for L in unique_labels:\n",
    "        best_cid = None\n",
    "        best_count = 0\n",
    "        for cid in cluster_ids_sorted:\n",
    "            c_count = cluster_label_counts[cid][L]\n",
    "            if c_count > best_count:\n",
    "                best_count = c_count\n",
    "                best_cid = cid\n",
    "        best_cluster_for_label[L] = (best_cid, best_count)\n",
    "\n",
    "    # 2) Compute per-label accuracy = (# L in best cluster) / (total # L)\n",
    "    print(\"\\nPer-Label Accuracy (Using Largest-Count Cluster):\")\n",
    "    label_accuracies = {}\n",
    "    overall_correct = 0\n",
    "    overall_total   = 0\n",
    "    for L in unique_labels:\n",
    "        best_cid, best_count_for_L = best_cluster_for_label[L]\n",
    "        total_L = total_label_count[L]\n",
    "        if total_L > 0:\n",
    "            acc_L = best_count_for_L / total_L\n",
    "        else:\n",
    "            acc_L = 0.0\n",
    "        label_accuracies[L] = acc_L\n",
    "        overall_correct += best_count_for_L\n",
    "        overall_total   += total_L\n",
    "        print(f\"  Label {L}: best_cluster={best_cid}, correct={best_count_for_L}, total={total_L}, acc={acc_L:.4f}\")\n",
    "\n",
    "    overall_system_acc = overall_correct / overall_total if overall_total else 0.0\n",
    "    print(f\"\\nOverall System Accuracy (summing per-label correct / total) = {overall_system_acc:.4f}\")\n",
    "\n",
    "    # 3) Count \"errors\" per cluster: i.e., images in each cluster that do NOT belong\n",
    "    #    to that cluster for their label's best cluster.\n",
    "    print(\"\\nPer-Cluster Error Counts:\")\n",
    "    cluster_error = {}\n",
    "    for cid in cluster_ids_sorted:\n",
    "        cluster_error[cid] = 0\n",
    "\n",
    "    # Map label -> correct cluster\n",
    "    correct_cluster_for_label = {}\n",
    "    for L, (cid, _) in best_cluster_for_label.items():\n",
    "        correct_cluster_for_label[L] = cid\n",
    "\n",
    "    # We'll go through each valid index again, see if label's correct cluster matches\n",
    "    for i in valid_indices:\n",
    "        c = cluster_ids[i]\n",
    "        lbl = labels[i]\n",
    "        # If this cluster c is not the correct cluster for label lbl => error\n",
    "        if c != correct_cluster_for_label[lbl]:\n",
    "            cluster_error[c] += 1\n",
    "\n",
    "    for cid in cluster_ids_sorted:\n",
    "        size_c = len(manager.cluster_buckets[cid])\n",
    "        err_c  = cluster_error[cid]\n",
    "        print(f\"  Cluster {cid}: size={size_c}, errors={err_c}, error_rate={err_c/size_c if size_c>0 else 0:.4f}\")\n",
    "\n",
    "    print(\"\\nDone evaluating class accuracy.\\n\")\n",
    "\n",
    "def plot_class_accuracy(manager):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of per-label accuracy using the \"largest-cluster\" approach.\n",
    "    For each label L, we find the cluster that contains the most of L, and treat\n",
    "    all L in that cluster as \"correct.\" Then we compute accuracy = correct / total(L).\n",
    "\n",
    "    NOTE: This function re-computes the label-cluster assignments internally, so\n",
    "    you don't need to call evaluate_class_accuracy beforehand.\n",
    "    \"\"\"\n",
    "\n",
    "    # Gather assigned (non-noise) indices\n",
    "    cluster_ids = manager.all_cluster_ids\n",
    "    labels      = manager.all_labels\n",
    "    valid_indices = np.where(cluster_ids != -1)[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        print(\"No non-noise assignments to evaluate.\")\n",
    "        return\n",
    "\n",
    "    assigned_clusters = cluster_ids[valid_indices]\n",
    "    assigned_labels   = labels[valid_indices]\n",
    "\n",
    "    # Build cluster->(label counts)\n",
    "    cluster_label_counts = defaultdict(Counter)\n",
    "    for i, c in zip(valid_indices, assigned_clusters):\n",
    "        lbl = labels[i]\n",
    "        cluster_label_counts[c][lbl] += 1\n",
    "\n",
    "    # Count total occurrences of each label\n",
    "    total_label_count = Counter(assigned_labels)\n",
    "\n",
    "    # For each label => best cluster\n",
    "    unique_labels = sorted(set(assigned_labels))\n",
    "    best_cluster_for_label = {}\n",
    "    for L in unique_labels:\n",
    "        best_cid = None\n",
    "        best_count = 0\n",
    "        for cid, lbl_counter in cluster_label_counts.items():\n",
    "            c_count = lbl_counter[L]\n",
    "            if c_count > best_count:\n",
    "                best_count = c_count\n",
    "                best_cid   = cid\n",
    "        best_cluster_for_label[L] = (best_cid, best_count)\n",
    "\n",
    "    # Compute per-label accuracy\n",
    "    label_accuracies = {}\n",
    "    for L in unique_labels:\n",
    "        best_cid, best_count_for_L = best_cluster_for_label[L]\n",
    "        total_L = total_label_count[L]\n",
    "        if total_L > 0:\n",
    "            label_acc = best_count_for_L / total_L\n",
    "        else:\n",
    "            label_acc = 0.0\n",
    "        label_accuracies[L] = label_acc\n",
    "\n",
    "    # Plot a bar chart of these per-label accuracies\n",
    "    lbls_sorted = sorted(label_accuracies.keys())\n",
    "    acc_values  = [label_accuracies[L] for L in lbls_sorted]\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar([str(lbl) for lbl in lbls_sorted], acc_values, color='purple')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel(\"Label (Class)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Per-Label Accuracy (Largest Cluster Assignment)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print numeric results & overall accuracy\n",
    "    overall_correct = 0\n",
    "    overall_total   = 0\n",
    "    for L in lbls_sorted:\n",
    "        best_cid, best_count_for_L = best_cluster_for_label[L]\n",
    "        total_L = total_label_count[L]\n",
    "        overall_correct += best_count_for_L\n",
    "        overall_total   += total_L\n",
    "        print(f\"Label {L}: best_cluster={best_cid}, correct={best_count_for_L}, total={total_L}, acc={label_accuracies[L]:.4f}\")\n",
    "\n",
    "    overall_acc = overall_correct / overall_total if overall_total > 0 else 0.0\n",
    "    print(f\"\\nOverall Accuracy (sum of per-label correct / total) = {overall_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "evaluate_class_accuracy(manager)\n",
    "plot_class_accuracy(manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
